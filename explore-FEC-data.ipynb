{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore FEC data.\n",
    "#### Find out how many federal court judges have donated to political campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pandas_ods_reader import read_ods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine lists of Art. III and bankruptcy/magistrate judges and get postal code abbreviations for court.\n",
    "- Fails to pick up anything other than judges in federal district or bankruptcy courts.\n",
    "- Stripping out suffixes may be problematic as it may match a \"Jr.\" to a \"Sr.\"\n",
    "- There are entries like \"Mrs. Tom Smith\" and stripping out \"Mrs.\" will result in false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general flag stating whether or not middle names will be used.  \"not_middle=True\" means middle names\n",
    "# will not be used to match.\n",
    "not_middle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_art3():\n",
    "    \"\"\"\n",
    "    Reduce a given court name to the postal code abbreviation of the state.\n",
    "    \"\"\"\n",
    "    state_dict = {'Alabama': 'AL',\n",
    "                  'Alaska': 'AK',\n",
    "                  'Arizona': 'AZ',\n",
    "                  'Arkansas': 'AR',\n",
    "                  'California': 'CA',\n",
    "                  'Colorado': 'CO',\n",
    "                  'Connecticut': 'CT',\n",
    "                  'Delaware': 'DE',\n",
    "                  'Florida': 'FL',\n",
    "                  'Georgia': 'GA',\n",
    "                  'Hawaii': 'HI',\n",
    "                  'Idaho': 'ID',\n",
    "                  'Illinois': 'IL',\n",
    "                  'Indiana': 'IN',\n",
    "                  'Iowa': 'IA',\n",
    "                  'Kansas': 'KS',\n",
    "                  'Kentucky': 'KY',\n",
    "                  'Louisiana': 'LA',\n",
    "                  'Maine': 'ME',\n",
    "                  'Maryland': 'MD',\n",
    "                  'Massachusetts': 'MA',\n",
    "                  'Michigan': 'MI',\n",
    "                  'Minnesota': 'MN',\n",
    "                  'Mississippi': 'MS',\n",
    "                  'Missouri': 'MO',\n",
    "                  'Montana': 'MT',\n",
    "                  'Nebraska': 'NE',\n",
    "                  'Nevada': 'NV',\n",
    "                  'New Hampshire': 'NH',\n",
    "                  'New Jersey': 'NJ',\n",
    "                  'New Mexico': 'NM',\n",
    "                  'New York': 'NY',\n",
    "                  'North Carolina': 'NC',\n",
    "                  'North Dakota': 'ND',\n",
    "                  'Ohio': 'OH',\n",
    "                  'Oklahoma': 'OK',\n",
    "                  'Oregon': 'OR',\n",
    "                  'Pennsylvania': 'PA',\n",
    "                  'Rhode Island': 'RI',\n",
    "                  'South Carolina': 'SC',\n",
    "                  'South Dakota': 'SD',\n",
    "                  'Tennessee': 'TN',\n",
    "                  'Texas': 'TX',\n",
    "                  'Utah': 'UT',\n",
    "                  'Vermont': 'VT',\n",
    "                  'Virginia': 'VA',\n",
    "                  'Washington': 'WA',\n",
    "                  'West Virginia': 'WV',\n",
    "                  'Wisconsin': 'WI',\n",
    "                  'Wyoming': 'WY',}\n",
    "    \n",
    "    art3 = pd.read_csv('art_3.csv')\n",
    "    art3 = art3[['First Name', 'Middle Name', 'Last Name', 'Court Name (1)']]\n",
    "    \n",
    "    art3 = art3.rename(columns={'First Name': 'NAME_FIRST',\n",
    "                                'Middle Name': 'NAME_MIDDLE',\n",
    "                                'Last Name': 'NAME_LAST',\n",
    "                                'Court Name (1)': 'COURT'})\n",
    "    \n",
    "    pattern = '|'.join(['U.S. District Court for the ', \n",
    "                        'Eastern District of ', \n",
    "                        'Western District of ',\n",
    "                        'Southern District of ',\n",
    "                        'Northern District of ',\n",
    "                        'Central District of ',\n",
    "                        'Middle District of ',\n",
    "                        'District of '])\n",
    "    \n",
    "    art3['COURT'] = art3['COURT'].str.replace(pattern, '')\n",
    "    art3['COURT'] = art3['COURT'].map(state_dict)\n",
    "    return art3\n",
    "\n",
    "\n",
    "def clean_bnk_mag():\n",
    "    \"\"\"\n",
    "    Read bankruptcy/magistrate sheet.\n",
    "    \"\"\"\n",
    "    bnk_mag = read_ods('bnk_mag.ods', 'data')\n",
    "    bnk_mag = bnk_mag[['NAME_FIRST', 'NAME_MIDDLE', 'NAME_LAST', 'COURT']]\n",
    "    bnk_mag['COURT'] = bnk_mag['COURT'].str.split(',', expand=True)\n",
    "    return bnk_mag\n",
    "\n",
    "\n",
    "def combine_judge_files():\n",
    "    \"\"\"\n",
    "    Combine Article III and bankruptcy/magistrate sheets.  Transform courts to postal codes.  \n",
    "    Add a column comprised of \"lastname_firstname_state\" to match with FEC data.\n",
    "    \"\"\"\n",
    "    art3 = clean_art3()\n",
    "    bnk_mag = clean_bnk_mag()\n",
    "    df = pd.concat([art3, bnk_mag], ignore_index=True)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.applymap(lambda x: x.strip().upper().strip('.') if isinstance(x, str) else x)\n",
    "    if not_middle:\n",
    "        df['MATCH_STR'] = df['NAME_LAST'] + \"_\" + df['NAME_FIRST'] + \"_\" + df['COURT']\n",
    "    else:\n",
    "        df['MATCH_STR'] = df['NAME_LAST'] + \"_\" + df['NAME_FIRST'] + \"_\" + df['NAME_MIDDLE'] + \"_\" + df['COURT']\n",
    "    # Dropping any judge that did not return a match string (which needs a state, which Circuit and others don't have.)\n",
    "    df = df.loc[df['MATCH_STR'].notnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "judges = combine_judge_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fec_names(file_path):\n",
    "    \"\"\"\n",
    "    Read FEC individual files and test for presence of judge.  Return results that match.\n",
    "    \"\"\"\n",
    "\n",
    "    indiv_vars = ['CMTE_ID', \n",
    "                  'AMNDT_IND', \n",
    "                  'RPT_TP', \n",
    "                  'TRANSACTION_PGI', \n",
    "                  'IMAGE_NUM', \n",
    "                  'TRANSACTION_TP', \n",
    "                  'ENTITY_TP', \n",
    "                  'NAME',\n",
    "                  'CITY', \n",
    "                  'STATE', \n",
    "                  'ZIP_CODE', \n",
    "                  'EMPLOYER', \n",
    "                  'OCCUPATION', \n",
    "                  'TRANSACTION_DT', \n",
    "                  'TRANSACTION_AMT', \n",
    "                  'OTHER_ID',\n",
    "                  'TRAN_ID', \n",
    "                  'FILE_NUM', \n",
    "                  'MEMO_CD', \n",
    "                  'MEMO_TEXT', \n",
    "                  'SUB_ID',]\n",
    "    \n",
    "    var_types = {'CMTE_ID': str, \n",
    "                 'AMNDT_IND': str, \n",
    "                 'RPT_TP': str, \n",
    "                 'TRANSACTION_PGI': str, \n",
    "                 'IMAGE_NUM': str, \n",
    "                 'TRANSACTION_TP': str, \n",
    "                 'ENTITY_TP': str, \n",
    "                 'NAME': str,\n",
    "                 'CITY': str, \n",
    "                 'STATE': str, \n",
    "                 'ZIP_CODE': str, \n",
    "                 'EMPLOYER': str, \n",
    "                 'OCCUPATION': str, \n",
    "                 'TRANSACTION_DT': str, \n",
    "                 'TRANSACTION_AMT': str, \n",
    "                 'OTHER_ID': str, \n",
    "                 'TRAN_ID': str, \n",
    "                 'FILE_NUM': str, \n",
    "                 'MEMO_CD': str, \n",
    "                 'MEMO_TEXT': str, \n",
    "                 'SUB_ID': str,}\n",
    "\n",
    "    pattern = '|'.join(['MR.',\n",
    "                        'MS.',\n",
    "                        'MRS.',\n",
    "                        'DR.',\n",
    "                        'JR',\n",
    "                        'SR.',\n",
    "                        'III',])\n",
    "    \n",
    "    chunk_list = []\n",
    "\n",
    "    for df in pd.read_csv(file_path, sep='|', names=indiv_vars, encoding='latin1', dtype=var_types, chunksize=100000):\n",
    "        df['ALT_NAME'] = df['NAME'].str.upper().str.strip()\n",
    "        # An example entry:  \"Chiles, Earle M.\"\n",
    "        if not_middle:\n",
    "            df['ALT_NAME'] = df['ALT_NAME'].str.replace(', ', '_')\n",
    "            df['ALT_NAME'] = df['ALT_NAME'].str.split(' ').str[0]\n",
    "        else:\n",
    "            df['ALT_NAME'] = df['ALT_NAME'].str.replace(pattern, '')\n",
    "            df['ALT_NAME'] = df['ALT_NAME'].str.replace(', ', '_')\n",
    "            df['ALT_NAME'] = df['ALT_NAME'].str.replace(' ', '_')\n",
    "        df['MATCH_STR'] = df['ALT_NAME'] + '_' + df['STATE'].str.strip() \n",
    "        df = df.loc[df['MATCH_STR'].isin(judges['MATCH_STR'])]\n",
    "        df['FILE'] = file_path\n",
    "        chunk_list.append(df)\n",
    "\n",
    "    df_all = pd.concat(chunk_list, ignore_index=True)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_fec_files():\n",
    "    \"\"\"\n",
    "    Concatenate FEC files, subsetting by MATCH_STR as we go along.\n",
    "    \"\"\"\n",
    "\n",
    "    fec_file_list = glob.glob('../data/individual-unzip/*')\n",
    "    df_list = map(get_fec_names, fec_file_list)\n",
    "    df_all = pd.concat(df_list, ignore_index=True)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fec = concat_fec_files()\n",
    "# 1981 results (not_middle=False)\n",
    "# fec.to_csv('first-middle-last-state.csv', index=False)\n",
    "# 58215 results (not_middle=True)\n",
    "fec.to_csv('first-last-state.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match individual files from above with candidates.\n",
    "- If concatenating two files generated above, need to remove duplicates from merged df.\n",
    "- Committee could support more than one candidate and can work across election cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of CMTE_IDs from individual data files.  We use this to subset the committee-candidates files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of CMTE_IDs: 58215\n",
      "Set of CMTE_IDs: 4978\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('first-last-state.csv')\n",
    "# Create set of all CMTE_IDs.\n",
    "comm_list = df['CMTE_ID'].tolist()\n",
    "print(\"List of CMTE_IDs:\", len(comm_list))\n",
    "comm_set = set(comm_list)\n",
    "print(\"Set of CMTE_IDs:\", len(comm_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a subset containing relevant entries from committee-candidate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comm_cand(file_path):\n",
    "    \n",
    "    comm_cand_vars = ['CAND_ID',\n",
    "                      'CAND_ELECTION_YR', \n",
    "                      'FEC_ELECTION_YR', \n",
    "                      'CMTE_ID', \n",
    "                      'CMTE_TP', \n",
    "                      'CMTE_DSGN', \n",
    "                      'LINKAGE_ID', \n",
    "                     ]\n",
    "    \n",
    "    df = pd.read_csv(file_path, \n",
    "                     sep='|',\n",
    "                     names=comm_cand_vars, \n",
    "                     encoding='latin1', \n",
    "                    )\n",
    "    \n",
    "    df = df.loc[df['CMTE_ID'].isin(comm_set)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df of all relevant committee-candidate linkages.\n",
    "fec_file_list = glob.glob('../data/cand-comm/*')\n",
    "df_list = map(get_comm_cand, fec_file_list)\n",
    "all_comm = pd.concat(df_list, ignore_index=True)\n",
    "all_comm.to_csv('get-candidate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get relevant candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate(file_path):\n",
    "    \n",
    "    cand_vars = ['CAND_ID', \n",
    "                 'CAND_NAME', \n",
    "                 'CAND_PTY_AFFILIATION', \n",
    "                 'CAND_ELECTION_YR', \n",
    "                 'CAND_OFFICE_ST', \n",
    "                 'CAND_OFFICE', \n",
    "                 'CAND_OFFICE_DISTRICT', \n",
    "                 'CAND_ICI', \n",
    "                 'CAND_STATUS', \n",
    "                 'CAND_PCC', \n",
    "                 'CAND_ST1', \n",
    "                 'CAND_ST2', \n",
    "                 'CAND_CITY', \n",
    "                 'CAND_ST', \n",
    "                 'CAND_ZIP', \n",
    "                ]\n",
    "    \n",
    "    df = pd.read_csv(file_path, \n",
    "                     sep='|',\n",
    "                     names=cand_vars, \n",
    "                     encoding='latin1', \n",
    "                    )\n",
    "    \n",
    "    df = df.loc[df['CAND_ID'].isin(all_comm['CAND_ID'])]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df of all relevant committee-candidate linkages.\n",
    "fec_file_list = glob.glob('../data/candidate/*')\n",
    "df_list = map(get_candidate, fec_file_list)\n",
    "all_cand = pd.concat(df_list, ignore_index=True)\n",
    "all_cand.to_csv('all-candidate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge individual, candidate, and candidate-committee files together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('all-candidate.csv')\n",
    "df1['YEAR'] = df1['CAND_ELECTION_YR'].astype(str)\n",
    "df1['YEAR'] = df1['YEAR'].str[2:]\n",
    "\n",
    "df2 = pd.read_csv('get-candidate.csv')\n",
    "df2['YEAR'] = df2['CAND_ELECTION_YR'].astype(str)\n",
    "df2['YEAR'] = df2['YEAR'].str[2:]\n",
    "\n",
    "df3 = pd.read_csv('first-last-state.csv')\n",
    "df3['YEAR'] = df['FILE'].str.split(\"/\").str[-1]\n",
    "df3['YEAR'] = df3['YEAR'].str.replace(\"indiv\", \"\").str.replace(\".txt\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of obs. in comm-cand file:        15298\n",
      "# of obs. in cand file:             11404\n",
      "# of obs. in first-last-state file: 58215\n"
     ]
    }
   ],
   "source": [
    "print(\"# of obs. in comm-cand file:       \", len(df1))\n",
    "print(\"# of obs. in cand file:            \", len(df2))\n",
    "print(\"# of obs. in first-last-state file:\", len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = pd.merge(df3, df2, how='left', on=['CMTE_ID', 'YEAR'])\n",
    "# comm-cand linkage only works for year 2000 or greater \"FEC_ELECTION_YR\"\n",
    "# This still generates duplicates though.\n",
    "merge1 = merge1[merge1['CAND_ID'].notnull()]\n",
    "merge1.to_csv('merge1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         CMTE_ID AMNDT_IND RPT_TP TRANSACTION_PGI           IMAGE_NUM  \\\n",
      "0      C00263343         A     YE             NaN         94015111322   \n",
      "1      C00263343         A     YE             NaN         94015111322   \n",
      "2      C00263343         A     YE             NaN         94015111322   \n",
      "3      C00263343         A     YE             NaN         94015111322   \n",
      "54     C00252601         A     YE             NaN         94015124985   \n",
      "...          ...       ...    ...             ...                 ...   \n",
      "80333  C00578013         A    12P           P2016  201607280200325237   \n",
      "80335  C00608398         A    30R           R2016  201701130200011840   \n",
      "80336  C00608398         A    30R           R2016  201701130200011948   \n",
      "80337  C00608398         A    30R           P2016  201703140200080965   \n",
      "80338  C00608398         A    30R           P2016  201703140200081073   \n",
      "\n",
      "      TRANSACTION_TP ENTITY_TP                NAME         CITY STATE  ...  \\\n",
      "0                 15       NaN     SCHILLER, BERLE       MERION    PA  ...   \n",
      "1                 15       NaN     SCHILLER, BERLE       MERION    PA  ...   \n",
      "2                 15       NaN     SCHILLER, BERLE       MERION    PA  ...   \n",
      "3                 15       NaN     SCHILLER, BERLE       MERION    PA  ...   \n",
      "54                15       NaN     NELSON, MICHAEL        OMAHA    NE  ...   \n",
      "...              ...       ...                 ...          ...   ...  ...   \n",
      "80333            15E       IND       HOWE, JAMES P       BELOIT    WI  ...   \n",
      "80335             15       IND       ASHE, BARRY W  NEW ORLEANS    LA  ...   \n",
      "80336             15       IND  HEEBE, FREDERICK R  NEW ORLEANS    LA  ...   \n",
      "80337             15       IND       ASHE, BARRY W  NEW ORLEANS    LA  ...   \n",
      "80338             15       IND  HEEBE, FREDERICK R  NEW ORLEANS    LA  ...   \n",
      "\n",
      "              ALT_NAME           MATCH_STR  \\\n",
      "0       SCHILLER_BERLE   SCHILLER_BERLE_PA   \n",
      "1       SCHILLER_BERLE   SCHILLER_BERLE_PA   \n",
      "2       SCHILLER_BERLE   SCHILLER_BERLE_PA   \n",
      "3       SCHILLER_BERLE   SCHILLER_BERLE_PA   \n",
      "54      NELSON_MICHAEL   NELSON_MICHAEL_NE   \n",
      "...                ...                 ...   \n",
      "80333       HOWE_JAMES       HOWE_JAMES_WI   \n",
      "80335       ASHE_BARRY       ASHE_BARRY_LA   \n",
      "80336  HEEBE_FREDERICK  HEEBE_FREDERICK_LA   \n",
      "80337       ASHE_BARRY       ASHE_BARRY_LA   \n",
      "80338  HEEBE_FREDERICK  HEEBE_FREDERICK_LA   \n",
      "\n",
      "                                       FILE  YEAR    CAND_ID CAND_ELECTION_YR  \\\n",
      "0      ../data/individual-unzip/indiv94.txt    94  H2PA13078           1994.0   \n",
      "1      ../data/individual-unzip/indiv94.txt    94  H2PA13078           1994.0   \n",
      "2      ../data/individual-unzip/indiv94.txt    94  H2PA13078           1994.0   \n",
      "3      ../data/individual-unzip/indiv94.txt    94  H2PA13078           1994.0   \n",
      "54     ../data/individual-unzip/indiv94.txt    94  H8NE02055           1994.0   \n",
      "...                                     ...   ...        ...              ...   \n",
      "80333  ../data/individual-unzip/indiv16.txt    16  S8WI00026           2016.0   \n",
      "80335  ../data/individual-unzip/indiv16.txt    16  S4LA00065           2016.0   \n",
      "80336  ../data/individual-unzip/indiv16.txt    16  S4LA00065           2016.0   \n",
      "80337  ../data/individual-unzip/indiv16.txt    16  S4LA00065           2016.0   \n",
      "80338  ../data/individual-unzip/indiv16.txt    16  S4LA00065           2016.0   \n",
      "\n",
      "      FEC_ELECTION_YR  CMTE_TP CMTE_DSGN LINKAGE_ID  \n",
      "0              2000.0        H         P    19273.0  \n",
      "1              2002.0        H         P    19274.0  \n",
      "2              2000.0        H         P    19273.0  \n",
      "3              2002.0        H         P    19274.0  \n",
      "54             2000.0        H         P    55274.0  \n",
      "...               ...      ...       ...        ...  \n",
      "80333          2016.0        S         P   209430.0  \n",
      "80335          2016.0        S         P   210699.0  \n",
      "80336          2016.0        S         P   210699.0  \n",
      "80337          2016.0        S         P   210699.0  \n",
      "80338          2016.0        S         P   210699.0  \n",
      "\n",
      "[41741 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(merge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
